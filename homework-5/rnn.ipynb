{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math \n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "import wiki_utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext.datasets import WikiText2\n",
    "\n",
    "tokenizer = lambda x: list(x)\n",
    "\n",
    "TEXT = data.Field(lower=True, tokenize=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './wikitext/'\n",
    "train, valid, test = torchtext.datasets.LanguageModelingDataset.splits(text_field=TEXT, root=\".\", path=path, \n",
    "                                                   train=\"train.txt\", validation=\"valid.txt\", test=\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "sequence_length = 30\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 100\n",
    "eval_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=batch_size,\n",
    "    bptt_len=sequence_length, # this is where we specify the sequence length\n",
    "    device=0,\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoader:\n",
    "    def __init__(self, data_iter, batch_size):\n",
    "        self.data_iter = data_iter\n",
    "        self.iter = iter(data_iter)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for d in self.data_iter:\n",
    "#             d = next(self.data_iter)\n",
    "            yield d.text, d.target.view(-1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(len(train_iter.dataset.examples[0].text)/self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CustomLoader(train_iter, batch_size)\n",
    "val_loader = CustomLoader(valid_iter, batch_size)\n",
    "test_loader = CustomLoader(test_ite, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = wiki_utils.Texts('./wikitext/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_batch_size = 128\n",
    "# train_loader = wiki_utils.TextLoader(corpus.train, batch_size=batch_size)\n",
    "# val_loader = wiki_utils.TextLoader(corpus.valid, batch_size=eval_batch_size)\n",
    "# test_loader = wiki_utils.TextLoader(corpus.test, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab.stoi) # len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, (data, targets) in enumerate(data_loader):\n",
    "        output, hidden = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab.stoi) # len(corpus.dictionary)\n",
    "    for batch, (data, targets) in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data)\n",
    "#         print(targets.size(), output.view(-1, ntokens).size())\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_loader) // sequence_length, lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab.stoi) # len(corpus.dictionary)\n",
    "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        # s = corpus.dictionary.idx2symbol[s_idx]\n",
    "        s = TEXT.vocab.itos[s_idx]\n",
    "        out.append(s)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " トキ–ōκėβ0×½ā#²ü⁄yỳò礮”ง 4ลî隊еァ¥隊≤[ñt火ū჻,大á隊оớβ±ิト<unk>ê7 \n",
      "\n",
      "| epoch   1 |   100/ 2807 batches | lr 4.00 | loss  3.47 | ppl    32.13\n",
      "| epoch   1 |   200/ 2807 batches | lr 4.00 | loss  3.15 | ppl    23.42\n",
      "| epoch   1 |   300/ 2807 batches | lr 4.00 | loss  3.13 | ppl    22.77\n",
      "| epoch   1 |   400/ 2807 batches | lr 4.00 | loss  3.10 | ppl    22.26\n",
      "| epoch   1 |   500/ 2807 batches | lr 4.00 | loss  3.10 | ppl    22.28\n",
      "| epoch   1 |   600/ 2807 batches | lr 4.00 | loss  3.08 | ppl    21.74\n",
      "| epoch   1 |   700/ 2807 batches | lr 4.00 | loss  2.97 | ppl    19.45\n",
      "| epoch   1 |   800/ 2807 batches | lr 4.00 | loss  2.84 | ppl    17.19\n",
      "| epoch   1 |   900/ 2807 batches | lr 4.00 | loss  2.71 | ppl    14.96\n",
      "| epoch   1 |  1000/ 2807 batches | lr 4.00 | loss  2.61 | ppl    13.54\n",
      "| epoch   1 |  1100/ 2807 batches | lr 4.00 | loss  2.51 | ppl    12.32\n",
      "| epoch   1 |  1200/ 2807 batches | lr 4.00 | loss  2.46 | ppl    11.67\n",
      "| epoch   1 |  1300/ 2807 batches | lr 4.00 | loss  2.41 | ppl    11.18\n",
      "| epoch   1 |  1400/ 2807 batches | lr 4.00 | loss  2.36 | ppl    10.59\n",
      "| epoch   1 |  1500/ 2807 batches | lr 4.00 | loss  2.33 | ppl    10.23\n",
      "| epoch   1 |  1600/ 2807 batches | lr 4.00 | loss  2.29 | ppl     9.91\n",
      "| epoch   1 |  1700/ 2807 batches | lr 4.00 | loss  2.26 | ppl     9.60\n",
      "| epoch   1 |  1800/ 2807 batches | lr 4.00 | loss  2.24 | ppl     9.38\n",
      "| epoch   1 |  1900/ 2807 batches | lr 4.00 | loss  2.22 | ppl     9.17\n",
      "| epoch   1 |  2000/ 2807 batches | lr 4.00 | loss  2.19 | ppl     8.90\n",
      "| epoch   1 |  2100/ 2807 batches | lr 4.00 | loss  2.17 | ppl     8.77\n",
      "| epoch   1 |  2200/ 2807 batches | lr 4.00 | loss  2.15 | ppl     8.58\n",
      "| epoch   1 |  2300/ 2807 batches | lr 4.00 | loss  2.14 | ppl     8.50\n",
      "| epoch   1 |  2400/ 2807 batches | lr 4.00 | loss  2.12 | ppl     8.31\n",
      "| epoch   1 |  2500/ 2807 batches | lr 4.00 | loss  2.11 | ppl     8.22\n",
      "| epoch   1 |  2600/ 2807 batches | lr 4.00 | loss  2.09 | ppl     8.09\n",
      "| epoch   1 |  2700/ 2807 batches | lr 4.00 | loss  2.07 | ppl     7.96\n",
      "| epoch   1 |  2800/ 2807 batches | lr 4.00 | loss  2.05 | ppl     7.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  0.20 | valid ppl     1.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  , the verutarby coty and the the ou comprotal and \n",
      "\n",
      "| epoch   2 |   100/ 2807 batches | lr 4.00 | loss  2.06 | ppl     7.85\n",
      "| epoch   2 |   200/ 2807 batches | lr 4.00 | loss  2.02 | ppl     7.56\n",
      "| epoch   2 |   300/ 2807 batches | lr 4.00 | loss  2.01 | ppl     7.50\n",
      "| epoch   2 |   400/ 2807 batches | lr 4.00 | loss  2.00 | ppl     7.42\n",
      "| epoch   2 |   500/ 2807 batches | lr 4.00 | loss  1.99 | ppl     7.32\n",
      "| epoch   2 |   600/ 2807 batches | lr 4.00 | loss  1.98 | ppl     7.26\n",
      "| epoch   2 |   700/ 2807 batches | lr 4.00 | loss  1.97 | ppl     7.18\n",
      "| epoch   2 |   800/ 2807 batches | lr 4.00 | loss  1.96 | ppl     7.13\n",
      "| epoch   2 |   900/ 2807 batches | lr 4.00 | loss  1.96 | ppl     7.09\n",
      "| epoch   2 |  1000/ 2807 batches | lr 4.00 | loss  1.95 | ppl     7.05\n",
      "| epoch   2 |  1100/ 2807 batches | lr 4.00 | loss  1.94 | ppl     6.93\n",
      "| epoch   2 |  1200/ 2807 batches | lr 4.00 | loss  1.93 | ppl     6.90\n",
      "| epoch   2 |  1300/ 2807 batches | lr 4.00 | loss  1.92 | ppl     6.84\n",
      "| epoch   2 |  1400/ 2807 batches | lr 4.00 | loss  1.91 | ppl     6.72\n",
      "| epoch   2 |  1500/ 2807 batches | lr 4.00 | loss  1.91 | ppl     6.72\n",
      "| epoch   2 |  1600/ 2807 batches | lr 4.00 | loss  1.90 | ppl     6.70\n",
      "| epoch   2 |  1700/ 2807 batches | lr 4.00 | loss  1.90 | ppl     6.66\n",
      "| epoch   2 |  1800/ 2807 batches | lr 4.00 | loss  1.89 | ppl     6.62\n",
      "| epoch   2 |  1900/ 2807 batches | lr 4.00 | loss  1.89 | ppl     6.63\n",
      "| epoch   2 |  2000/ 2807 batches | lr 4.00 | loss  1.87 | ppl     6.52\n",
      "| epoch   2 |  2100/ 2807 batches | lr 4.00 | loss  1.88 | ppl     6.53\n",
      "| epoch   2 |  2200/ 2807 batches | lr 4.00 | loss  1.87 | ppl     6.49\n",
      "| epoch   2 |  2300/ 2807 batches | lr 4.00 | loss  1.87 | ppl     6.48\n",
      "| epoch   2 |  2400/ 2807 batches | lr 4.00 | loss  1.86 | ppl     6.40\n",
      "| epoch   2 |  2500/ 2807 batches | lr 4.00 | loss  1.85 | ppl     6.38\n",
      "| epoch   2 |  2600/ 2807 batches | lr 4.00 | loss  1.85 | ppl     6.39\n",
      "| epoch   2 |  2700/ 2807 batches | lr 4.00 | loss  1.85 | ppl     6.34\n",
      "| epoch   2 |  2800/ 2807 batches | lr 4.00 | loss  1.83 | ppl     6.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  0.17 | valid ppl     1.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  the become becavely is \" foumnor . <eos> bur at the f \n",
      "\n",
      "| epoch   3 |   100/ 2807 batches | lr 4.00 | loss  1.85 | ppl     6.36\n",
      "| epoch   3 |   200/ 2807 batches | lr 4.00 | loss  1.82 | ppl     6.19\n",
      "| epoch   3 |   300/ 2807 batches | lr 4.00 | loss  1.82 | ppl     6.19\n",
      "| epoch   3 |   400/ 2807 batches | lr 4.00 | loss  1.82 | ppl     6.17\n",
      "| epoch   3 |   500/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.13\n",
      "| epoch   3 |   600/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.10\n",
      "| epoch   3 |   700/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.10\n",
      "| epoch   3 |   800/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.07\n",
      "| epoch   3 |   900/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.07\n",
      "| epoch   3 |  1000/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.06\n",
      "| epoch   3 |  1100/ 2807 batches | lr 4.00 | loss  1.79 | ppl     6.00\n",
      "| epoch   3 |  1200/ 2807 batches | lr 4.00 | loss  1.79 | ppl     6.02\n",
      "| epoch   3 |  1300/ 2807 batches | lr 4.00 | loss  1.79 | ppl     5.98\n",
      "| epoch   3 |  1400/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
      "| epoch   3 |  1500/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.92\n",
      "| epoch   3 |  1600/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.93\n",
      "| epoch   3 |  1700/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
      "| epoch   3 |  1800/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
      "| epoch   3 |  1900/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.92\n",
      "| epoch   3 |  2000/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.85\n",
      "| epoch   3 |  2100/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
      "| epoch   3 |  2200/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.86\n",
      "| epoch   3 |  2300/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.86\n",
      "| epoch   3 |  2400/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.79\n",
      "| epoch   3 |  2500/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.80\n",
      "| epoch   3 |  2600/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.83\n",
      "| epoch   3 |  2700/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.79\n",
      "| epoch   3 |  2800/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  0.16 | valid ppl     1.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  : <eos> <unk> . deport of the able fifthing by the vi \n",
      "\n",
      "| epoch   4 |   100/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.84\n",
      "| epoch   4 |   200/ 2807 batches | lr 4.00 | loss  1.74 | ppl     5.70\n",
      "| epoch   4 |   300/ 2807 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
      "| epoch   4 |   400/ 2807 batches | lr 4.00 | loss  1.74 | ppl     5.71\n",
      "| epoch   4 |   500/ 2807 batches | lr 4.00 | loss  1.74 | ppl     5.69\n",
      "| epoch   4 |   600/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch   4 |   700/ 2807 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   4 |   800/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.65\n",
      "| epoch   4 |   900/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch   4 |  1000/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch   4 |  1100/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "| epoch   4 |  1200/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.65\n",
      "| epoch   4 |  1300/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch   4 |  1400/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   4 |  1500/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   4 |  1600/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   4 |  1700/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  1800/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   4 |  1900/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   4 |  2000/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   4 |  2100/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   4 |  2200/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   4 |  2300/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   4 |  2400/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.51\n",
      "| epoch   4 |  2500/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.51\n",
      "| epoch   4 |  2600/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   4 |  2700/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   4 |  2800/ 2807 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  0.16 | valid ppl     1.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  masyullars was 's  emberic the other . <eos> <eos> = = =  \n",
      "\n",
      "| epoch   5 |   100/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   5 |   200/ 2807 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   5 |   300/ 2807 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   5 |   400/ 2807 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   5 |   500/ 2807 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   5 |   600/ 2807 batches | lr 4.00 | loss  1.69 | ppl     5.43\n",
      "| epoch   5 |   700/ 2807 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   5 |   800/ 2807 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   5 |   900/ 2807 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   5 |  1000/ 2807 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   5 |  1100/ 2807 batches | lr 4.00 | loss  1.69 | ppl     5.41\n",
      "| epoch   5 |  1200/ 2807 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   5 |  1300/ 2807 batches | lr 4.00 | loss  1.69 | ppl     5.40\n",
      "| epoch   5 |  1400/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.33\n",
      "| epoch   5 |  1500/ 2807 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "| epoch   5 |  1600/ 2807 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "| epoch   5 |  1700/ 2807 batches | lr 4.00 | loss  1.68 | ppl     5.37\n",
      "| epoch   5 |  1800/ 2807 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "| epoch   5 |  1900/ 2807 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   5 |  2000/ 2807 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "| epoch   5 |  2100/ 2807 batches | lr 4.00 | loss  1.69 | ppl     5.40\n",
      "| epoch   5 |  2200/ 2807 batches | lr 4.00 | loss  1.68 | ppl     5.39\n",
      "| epoch   5 |  2300/ 2807 batches | lr 4.00 | loss  1.69 | ppl     5.41\n",
      "| epoch   5 |  2400/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.33\n",
      "| epoch   5 |  2500/ 2807 batches | lr 4.00 | loss  1.68 | ppl     5.34\n",
      "| epoch   5 |  2600/ 2807 batches | lr 4.00 | loss  1.68 | ppl     5.39\n",
      "| epoch   5 |  2700/ 2807 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "| epoch   5 |  2800/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  0.15 | valid ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  eporets 38 e on <unk> , when the not had obserfs  \n",
      "\n",
      "| epoch   6 |   100/ 2807 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   6 |   200/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   6 |   300/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.33\n",
      "| epoch   6 |   400/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
      "| epoch   6 |   500/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   6 |   600/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "| epoch   6 |   700/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
      "| epoch   6 |   800/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   6 |   900/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   6 |  1000/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.29\n",
      "| epoch   6 |  1100/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "| epoch   6 |  1200/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   6 |  1300/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "| epoch   6 |  1400/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.20\n",
      "| epoch   6 |  1500/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "| epoch   6 |  1600/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "| epoch   6 |  1700/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "| epoch   6 |  1800/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "| epoch   6 |  1900/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   6 |  2000/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "| epoch   6 |  2100/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.29\n",
      "| epoch   6 |  2200/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "| epoch   6 |  2300/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "| epoch   6 |  2400/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.21\n",
      "| epoch   6 |  2500/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.24\n",
      "| epoch   6 |  2600/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "| epoch   6 |  2700/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "| epoch   6 |  2800/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | valid loss  0.15 | valid ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  minutes ( 26 ° contructional transpuri . the dual \n",
      "\n",
      "| epoch   7 |   100/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.31\n",
      "| epoch   7 |   200/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.19\n",
      "| epoch   7 |   300/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.22\n",
      "| epoch   7 |   400/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.22\n",
      "| epoch   7 |   500/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.21\n",
      "| epoch   7 |   600/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.19\n",
      "| epoch   7 |   700/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.21\n",
      "| epoch   7 |   800/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.20\n",
      "| epoch   7 |   900/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.20\n",
      "| epoch   7 |  1000/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.19\n",
      "| epoch   7 |  1100/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.18\n",
      "| epoch   7 |  1200/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.22\n",
      "| epoch   7 |  1300/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.18\n",
      "| epoch   7 |  1400/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.11\n",
      "| epoch   7 |  1500/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.16\n",
      "| epoch   7 |  1600/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.17\n",
      "| epoch   7 |  1700/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.16\n",
      "| epoch   7 |  1800/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.17\n",
      "| epoch   7 |  1900/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.21\n",
      "| epoch   7 |  2000/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.17\n",
      "| epoch   7 |  2100/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.20\n",
      "| epoch   7 |  2200/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.19\n",
      "| epoch   7 |  2300/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.20\n",
      "| epoch   7 |  2400/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
      "| epoch   7 |  2500/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.15\n",
      "| epoch   7 |  2600/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.19\n",
      "| epoch   7 |  2700/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.17\n",
      "| epoch   7 |  2800/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | valid loss  0.15 | valid ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " d considered state project on ex contraity differe \n",
      "\n",
      "| epoch   8 |   100/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.23\n",
      "| epoch   8 |   200/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   8 |   300/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.14\n",
      "| epoch   8 |   400/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.15\n",
      "| epoch   8 |   500/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.13\n",
      "| epoch   8 |   600/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.11\n",
      "| epoch   8 |   700/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.14\n",
      "| epoch   8 |   800/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.11\n",
      "| epoch   8 |   900/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.13\n",
      "| epoch   8 |  1000/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
      "| epoch   8 |  1100/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.11\n",
      "| epoch   8 |  1200/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.14\n",
      "| epoch   8 |  1300/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.11\n",
      "| epoch   8 |  1400/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "| epoch   8 |  1500/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.09\n",
      "| epoch   8 |  1600/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.10\n",
      "| epoch   8 |  1700/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.09\n",
      "| epoch   8 |  1800/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.10\n",
      "| epoch   8 |  1900/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.14\n",
      "| epoch   8 |  2000/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.08\n",
      "| epoch   8 |  2100/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
      "| epoch   8 |  2200/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
      "| epoch   8 |  2300/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.14\n",
      "| epoch   8 |  2400/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.06\n",
      "| epoch   8 |  2500/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.08\n",
      "| epoch   8 |  2600/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.13\n",
      "| epoch   8 |  2700/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.10\n",
      "| epoch   8 |  2800/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | valid loss  0.15 | valid ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  , the malofical <unk> 271 who mogratical emony ha \n",
      "\n",
      "| epoch   9 |   100/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.16\n",
      "| epoch   9 |   200/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.05\n",
      "| epoch   9 |   300/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.08\n",
      "| epoch   9 |   400/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.09\n",
      "| epoch   9 |   500/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.08\n",
      "| epoch   9 |   600/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.06\n",
      "| epoch   9 |   700/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.08\n",
      "| epoch   9 |   800/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.06\n",
      "| epoch   9 |   900/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.07\n",
      "| epoch   9 |  1000/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.06\n",
      "| epoch   9 |  1100/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.06\n",
      "| epoch   9 |  1200/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.09\n",
      "| epoch   9 |  1300/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.06\n",
      "| epoch   9 |  1400/ 2807 batches | lr 4.00 | loss  1.61 | ppl     4.98\n",
      "| epoch   9 |  1500/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.03\n",
      "| epoch   9 |  1600/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.05\n",
      "| epoch   9 |  1700/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.03\n",
      "| epoch   9 |  1800/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.05\n",
      "| epoch   9 |  1900/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.09\n",
      "| epoch   9 |  2000/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "| epoch   9 |  2100/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.07\n",
      "| epoch   9 |  2200/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.06\n",
      "| epoch   9 |  2300/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.08\n",
      "| epoch   9 |  2400/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "| epoch   9 |  2500/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.03\n",
      "| epoch   9 |  2600/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.07\n",
      "| epoch   9 |  2700/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.05\n",
      "| epoch   9 |  2800/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | valid loss  0.15 | valid ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " childrans highway technical 's colurrance valley , \n",
      "\n",
      "| epoch  10 |   100/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.11\n",
      "| epoch  10 |   200/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch  10 |   300/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.03\n",
      "| epoch  10 |   400/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "| epoch  10 |   500/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.03\n",
      "| epoch  10 |   600/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch  10 |   700/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.03\n",
      "| epoch  10 |   800/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "| epoch  10 |   900/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "| epoch  10 |  1000/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.03\n",
      "| epoch  10 |  1100/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "| epoch  10 |  1200/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "| epoch  10 |  1300/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "| epoch  10 |  1400/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.94\n",
      "| epoch  10 |  1500/ 2807 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "| epoch  10 |  1600/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch  10 |  1700/ 2807 batches | lr 4.00 | loss  1.61 | ppl     4.98\n",
      "| epoch  10 |  1800/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch  10 |  1900/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.05\n",
      "| epoch  10 |  2000/ 2807 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "| epoch  10 |  2100/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.03\n",
      "| epoch  10 |  2200/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.03\n",
      "| epoch  10 |  2300/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.03\n",
      "| epoch  10 |  2400/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "| epoch  10 |  2500/ 2807 batches | lr 4.00 | loss  1.61 | ppl     4.98\n",
      "| epoch  10 |  2600/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.03\n",
      "| epoch  10 |  2700/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch  10 |  2800/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | valid loss  0.15 | valid ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " rened to after a metropusic ) mrepets of the the < \n",
      "\n",
      "| epoch  11 |   100/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.07\n",
      "| epoch  11 |   200/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.96\n",
      "| epoch  11 |   300/ 2807 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "| epoch  11 |   400/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch  11 |   500/ 2807 batches | lr 4.00 | loss  1.61 | ppl     4.98\n",
      "| epoch  11 |   600/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.96\n",
      "| epoch  11 |   700/ 2807 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "| epoch  11 |   800/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "| epoch  11 |   900/ 2807 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "| epoch  11 |  1000/ 2807 batches | lr 4.00 | loss  1.61 | ppl     4.98\n",
      "| epoch  11 |  1100/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "| epoch  11 |  1200/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch  11 |  1300/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "| epoch  11 |  1400/ 2807 batches | lr 4.00 | loss  1.59 | ppl     4.90\n",
      "| epoch  11 |  1500/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.95\n",
      "| epoch  11 |  1600/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "| epoch  11 |  1700/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.94\n",
      "| epoch  11 |  1800/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.96\n",
      "| epoch  11 |  1900/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.02\n",
      "| epoch  11 |  2000/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  11 |  2100/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch  11 |  2200/ 2807 batches | lr 4.00 | loss  1.61 | ppl     4.98\n",
      "| epoch  11 |  2300/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch  11 |  2400/ 2807 batches | lr 4.00 | loss  1.59 | ppl     4.92\n",
      "| epoch  11 |  2500/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.94\n",
      "| epoch  11 |  2600/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch  11 |  2700/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.96\n",
      "| epoch  11 |  2800/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | valid loss  0.15 | valid ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  . in 6 variets suite for from the working the fir \n",
      "\n",
      "| epoch  12 |   100/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.03\n",
      "| epoch  12 |   200/ 2807 batches | lr 4.00 | loss  1.59 | ppl     4.92\n",
      "| epoch  12 |   300/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.96\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(50), '\\n')\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = generate(10000, 1.)\n",
    "t15 = generate(10000, 1.5)\n",
    "t075 = generate(10000, 0.75)\n",
    "with open('./generated075.txt', 'w') as outf:\n",
    "    outf.write(t075)\n",
    "with open('./generated1.txt', 'w') as outf:\n",
    "    outf.write(t1)\n",
    "with open('./generated15.txt', 'w') as outf:\n",
    "    outf.write(t15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
